{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c2cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5217c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_datasets, get_dataloaders\n",
    "from random_texts import CLIPZeroShotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6361c9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd11841976b94fea9c809805eb46742a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "15\n",
      "21\n",
      "51\n",
      "52\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "datasets, classnames = get_datasets(fraction=1e-3)\n",
    "for name, dataset in datasets.items():\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2189383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot classifier: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:29<00:00, 11.62it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_model = CLIPZeroShotClassifier(classnames)\n",
    "dataloaders = get_dataloaders(datasets, baseline_model.preprocess, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ec1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module) -> dict[str, float]:\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    with torch.inference_mode():\n",
    "        for name, dataloader in dataloaders.items():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch in tqdm(dataloader, desc=f\"Evaluating {name}\"):\n",
    "                images = batch[\"image\"]\n",
    "                labels = batch[\"label\"]\n",
    "                logits = model(images)\n",
    "                correct += (logits.argmax(dim=-1) == labels).float().sum()\n",
    "                total += len(labels)\n",
    "            results[name] = correct / total\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda08e19",
   "metadata": {},
   "source": [
    "## Zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393805d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ID: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.69s/it]\n",
      "Evaluating OOD_infograph: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Evaluating OOD_painting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Evaluating OOD_quickdraw: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.61s/it]\n",
      "Evaluating OOD_real: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.06s/it]\n",
      "Evaluating OOD_clipart: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': tensor(0.8417),\n",
      " 'OOD_clipart': tensor(0.6500),\n",
      " 'OOD_infograph': tensor(0.4000),\n",
      " 'OOD_painting': tensor(0.7619),\n",
      " 'OOD_quickdraw': tensor(0.1373),\n",
      " 'OOD_real': tensor(0.8269)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_results = evaluate(baseline_model)\n",
    "pprint(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa164074",
   "metadata": {},
   "source": [
    "## Full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca55f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:25<00:00, 13.67it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_model = CLIPZeroShotClassifier(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedef8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:25<00:00, 25.70s/it, loss=0.6562]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(ft_model.parameters(), lr=3e-5, weight_decay=0.1)\n",
    "total_steps = len(dataloaders[\"ID\"])\n",
    "warmup_steps = min(500, total_steps // 2)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    total_iters=warmup_steps,\n",
    ")\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - warmup_steps,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "pbar = tqdm(dataloaders[\"ID\"], desc=\"Fine-tuning\")\n",
    "for batch in pbar:\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    logits = ft_model(images)\n",
    "    loss = nn.functional.cross_entropy(logits, labels)\n",
    "    pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ft_model.pth\", \"wb\") as f:\n",
    "    torch.save(ft_model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eabc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ID: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Evaluating OOD_infograph: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Evaluating OOD_painting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Evaluating OOD_quickdraw: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.57s/it]\n",
      "Evaluating OOD_real: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.63s/it]\n",
      "Evaluating OOD_clipart: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': tensor(0.9083),\n",
      " 'OOD_clipart': tensor(0.6500),\n",
      " 'OOD_infograph': tensor(0.4000),\n",
      " 'OOD_painting': tensor(0.7619),\n",
      " 'OOD_quickdraw': tensor(0.1373),\n",
      " 'OOD_real': tensor(0.8269)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft_results = evaluate(ft_model)\n",
    "pprint(ft_results)\n",
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54f0e1",
   "metadata": {},
   "source": [
    "## Lipsum-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8221a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-shot classifier: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:23<00:00, 14.88it/s]\n"
     ]
    }
   ],
   "source": [
    "lipsum_model = CLIPZeroShotClassifier(classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c842b73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™ðŸ¼ lgbti wr âž¤ shedlight care surro\n",
      "kamal ctive tob ii mags proj acea emed \n",
      "buzz cur bredyap straight nigel peregravail \n",
      "leash memory machðŸ‡·ðŸ‡º happens domestdrome daz\n",
      "pkwy axelkon scary fault dana minus Ã­a \n",
      "pion issaseaworld hipsters accommodate processors grumpy lizards \n",
      "gzchand adic wec caterpillar charlesdren; \n",
      "reminding poppyyash snazzy maz pressing rahuundocumented \n",
      "speed sevxalbums umni enthreventoss \n",
      "photobomb millennium rift ruewifey icelandic iciff \n",
      "machinefiftycokeï¿½satishomerdebates howe \n",
      "alliancetins edged nation pilooklahoma leur partner\n",
      "ceiling dempsey standalone jdm sightings anchored caliber dder \n",
      "â–ªpent â­â­earts wahgenerates tered saver \n",
      "frampton bookofbettersickest routines mastâ¦â¦@ atoday \n",
      "berto saffron ffler sorrows miro ðŸ¤©bingashok \n"
     ]
    }
   ],
   "source": [
    "from clip.clip import _tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample_random_tokens(n: int, L: int = 8):\n",
    "    V = len(_tokenizer.encoder)\n",
    "    return [\n",
    "        \"\".join(_tokenizer.decode(np.random.randint(0, V, size=L))) for _ in range(n)\n",
    "    ]\n",
    "\n",
    "\n",
    "print(*sample_random_tokens(16), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38c2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.54s/it, ce_loss=0.6431, gap_loss=0.0000]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(lipsum_model.parameters(), lr=3e-5, weight_decay=0.1)\n",
    "total_steps = len(dataloaders[\"ID\"])\n",
    "warmup_steps = min(500, total_steps // 2)\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.1,\n",
    "    total_iters=warmup_steps,\n",
    ")\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - warmup_steps,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "lambda_lipsum = 0.1\n",
    "pbar = tqdm(dataloaders[\"ID\"], desc=\"Fine-tuning\")\n",
    "for batch in pbar:\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    texts = sample_random_tokens(len(images))\n",
    "    logits = lipsum_model(images)\n",
    "    cur_energy = lipsum_model.get_energy(images, texts)\n",
    "    with torch.no_grad():\n",
    "        old_energy = baseline_model.get_energy(images, texts)\n",
    "    ce_loss = nn.functional.cross_entropy(logits, labels)\n",
    "    gap_loss = nn.functional.mse_loss(cur_energy, old_energy)\n",
    "    loss = ce_loss + lambda_lipsum * gap_loss\n",
    "    pbar.set_postfix(ce_loss=f\"{ce_loss.item():.4f}\", gap_loss=f\"{gap_loss.item():.4f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bbf114",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lipsum_model.pth\", \"wb\") as f:\n",
    "    torch.save(lipsum_model.state_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
